{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # WK 1/Phase 1 - Ingestion and Cleaning\n",
    " ## Variable Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sys import platform\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_integer(x):\n",
    "    '''\n",
    "    This function returns True if x is an integer, and False otherwise\n",
    "    '''\n",
    "    try:\n",
    "        return (int(x) == float(x))\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir_data = \"C:\\\\Users\\\\rodol\\\\OneDrive\\\\Documents\\\\Training\\\\Carnegie_Mellon_University\\\\Courses\\\\Mini_7\\\\Business_Value_Analytics\\\\General\\\\WK1\\\\Data\"\n",
    "dir_data = \"C:\\\\Users\\\\ly266e\\\\Documents\\\\Training\\\\CMU\\\\Master\\\\Fall 2023 Mini 7\\\\Business_Analytics\\\\HW\\\\HW5\\\\Update\\\\Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 1 - Ingestion\n",
    " Ingest the data files from both sets, perform consistency checks, and prepare one single file for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, final\n",
    "def ingest_files(directory: str) -> Dict:\n",
    "    '''\n",
    "    This function will ingest every file in the specified directory\n",
    "    into a pandas dataframe. It will return a dictionary containing\n",
    "    these dataframes, keyed by the file name.\n",
    "    \n",
    "    We assume the directory contains files directly downloaded from\n",
    "    Lending Club, and *only* those files. Thus, we assume the files are zipped\n",
    "    (pd.read_csv can read zipped files) and we assume the first line\n",
    "    in each file needs to be skipped. \n",
    "    \n",
    "    Note that this function will read and ingest more than one file and is\n",
    "    convenient if you want to ingest data for more than one year at a time.\n",
    "    \n",
    "    Note that each file will be read *without* formatting\n",
    "    '''\n",
    "    \n",
    "    # If the directory has no trailing slash, add one\n",
    "    if directory[-1] != \"/\":\n",
    "        directory = directory + \"/\"\n",
    "    \n",
    "    all_files = os.listdir(directory)\n",
    "    output = {}\n",
    "    \n",
    "    print(\"Directory \" + directory + \" has \" + str(len(all_files)) + \" files:\")\n",
    "    for i in all_files:\n",
    "        print(\"    Reading file \" + i)\n",
    "        output[i] = pd.read_csv(directory + i, dtype = str, skiprows = 1)\n",
    "        \n",
    "        # Some of the files have \"summary\" lines that, for example\n",
    "        # read \"Total number of loans number in Policy 1: .....\"\n",
    "        # To remove those lines, find any lines with non-integer IDs\n",
    "        # and remove them\n",
    "        invalid_rows = (output[i].id.apply( lambda x : is_integer(x) == False ))\n",
    "        if invalid_rows.sum() > 0:\n",
    "            print(\"        Found \" + str(invalid_rows.sum()) + \" invalid rows which were removed\")\n",
    "            output[i] = output[i][invalid_rows == False]\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory C:\\Users\\ly266e\\Documents\\Training\\CMU\\Master\\Fall 2023 Mini 7\\Business_Analytics\\HW\\HW5\\Update\\Data/ has 1 files:\n",
      "    Reading file LoanStats3c.csv\n",
      "        Found 4 invalid rows which were removed\n"
     ]
    }
   ],
   "source": [
    "# Ingest the set of files we downloaded \n",
    "files_data = ingest_files(dir_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['LoanStats3c.csv'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_now = pd.concat(files_data.values()).reset_index(drop = True)\n",
    "columns = list(data_now.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of columns is: 144\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of columns is: {len(columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 235629 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting with \" + str(len(data_now)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 2 - Choose Columns and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the columns we'll be keeping from the dataset\n",
    "cols_to_pick = ['id','loan_amnt','funded_amnt','term','int_rate'\n",
    "                ,'grade','emp_length', 'home_ownership',\n",
    "                'annual_inc','verification_status','issue_d',\n",
    "                'loan_status','purpose','dti', 'delinq_2yrs','open_acc','pub_rec',\n",
    "                'revol_bal','revol_util', 'total_pymnt', 'recoveries',\n",
    "                'inq_last_6mths', 'pct_tl_nvr_dlq', 'last_pymnt_d', 'earliest_cr_line']\n",
    "\n",
    "# Identify the type of each of these column\n",
    "float_cols = ['loan_amnt', 'funded_amnt', 'annual_inc',\n",
    "              'dti', 'revol_bal', 'delinq_2yrs', 'open_acc', 'pub_rec',\n",
    "              'total_pymnt', 'recoveries', 'inq_last_6mths']\n",
    "\n",
    "cat_cols = ['term', 'grade', 'emp_length', 'home_ownership',\n",
    "                    'verification_status', 'loan_status', 'purpose']\n",
    "\n",
    "perc_cols = ['int_rate', 'revol_util', 'pct_tl_nvr_dlq']\n",
    "\n",
    "date_cols = ['issue_d', 'last_pymnt_d', 'earliest_cr_line']\n",
    "\n",
    "# Ensure that we have types for every column\n",
    "assert set(cols_to_pick) - set(float_cols) - set(cat_cols) - set(perc_cols) - set(date_cols) == set([\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the columns of interest\n",
    "final_data = data_now[cols_to_pick].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 235629 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting with \" + str(len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # To do (A)\n",
    "\n",
    " Choose 3 to 5 variables and add them to the list of variables below\n",
    "\n",
    " You should consult the data description (excel) file you downloaded to understand the definition of various available columns\n",
    "\n",
    " TIP: If you added new variables, be sure to clean them as we just did for the default variables.\n",
    "\n",
    " You will have to add them to the group of the right type of variables (e.g. percentage, date, categorical)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Typecast the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in float_cols:\n",
    "    final_data[i] = final_data[i].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "def clean_perc(x):\n",
    "    if pd.isnull(x):\n",
    "        return np.nan\n",
    "    \n",
    "    # If x is already a float (or int), return it as is\n",
    "    if isinstance(x, (float, int)):\n",
    "        return float(x)\n",
    "    \n",
    "    x = x.rstrip()\n",
    "    if x.endswith('%'):\n",
    "        x = x[:-1]\n",
    "        \n",
    "    if x == '':\n",
    "        return np.nan\n",
    "    else:\n",
    "        return float(x)\n",
    "    \n",
    "for i in perc_cols:\n",
    "    final_data[i] = final_data[i].apply( clean_perc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "def clean_date(x):\n",
    "    if pd.isnull(x):\n",
    "        return None\n",
    "    else:\n",
    "        return datetime.datetime.strptime(x, \"%b-%y\").date()\n",
    "\n",
    "# Assuming final_data is your DataFrame and date_cols contains the date columns\n",
    "for i in date_cols:\n",
    "    final_data[i] = final_data[i].apply(clean_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "for i in cat_cols:\n",
    "    final_data.loc[final_data[i].isnull(), i] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows so far 235629\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Rows so far \" + str(len(final_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 4 - Handle outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 78 rows\n"
     ]
    }
   ],
   "source": [
    "# There are quite a few outliers, but the two most obvious\n",
    "# ones to remove are in annual_inc, revol_bal Remove these.\n",
    "n_rows = len(final_data)\n",
    "final_data = final_data[final_data.annual_inc < 1000000]\n",
    "final_data = final_data[final_data.revol_bal < 400000]\n",
    "final_data = final_data[final_data.dti < 200]\n",
    "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 rows\n"
     ]
    }
   ],
   "source": [
    "# Remove all loans that are too recent to have been paid off or defaulted\n",
    "n_rows = len(final_data)\n",
    "final_data = final_data[final_data.loan_status.isin(['Fully Paid','Charged Off','Default'])]\n",
    "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 42644 rows\n"
     ]
    }
   ],
   "source": [
    "# Only include loans issued since 2009\n",
    "n_rows = len(final_data)\n",
    "final_data = final_data[final_data.issue_d >= datetime.date(2009, 1, 1)]\n",
    "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Outlier detection and Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_and_limit_outliers(df, cols, z_threshold=3):\n",
    "    from scipy import stats\n",
    "    \n",
    "    \"\"\"\n",
    "    Flag outliers in specified columns using Z-score method and return the upper limit.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): Input DataFrame\n",
    "        cols (list): List of columns to check for outliers\n",
    "        z_threshold (float): Z-score threshold for flagging outliers. Defaults to 3.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with additional columns to flag outliers\n",
    "        dict: Dictionary with upper limits for each column\n",
    "    \"\"\"\n",
    "    df_outliers = df.copy()\n",
    "    upper_limits = {}\n",
    "    \n",
    "    for col in cols:\n",
    "        col_mean = df_outliers[col].mean()\n",
    "        col_std = df_outliers[col].std()\n",
    "        \n",
    "        # Calculate upper limit for each column\n",
    "        upper_limit = col_mean + (z_threshold * col_std)\n",
    "        upper_limits[col] = upper_limit\n",
    "        \n",
    "        # Flag outliers in DataFrame\n",
    "        df_outliers[f\"{col}_outlier\"] = (df_outliers[col] > upper_limit).astype(int)\n",
    "        \n",
    "    return df_outliers, upper_limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = ['annual_inc', 'inq_last_6mths', 'total_pymnt', 'open_acc']\n",
    "\n",
    "# Flag outliers and get upper limits\n",
    "df_outliers_flagged, upper_limits = flag_and_limit_outliers(final_data, columns_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_columns = ['annual_inc', 'inq_last_6mths', 'total_pymnt', 'open_acc']\n",
    "upper_limits_filtered = {k: upper_limits[k] for k in outliers_columns if k in upper_limits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 11169 rows\n"
     ]
    }
   ],
   "source": [
    "# Remove outliers based on upper limits\n",
    "n_rows = len(final_data)\n",
    "for col, upper_limit in upper_limits_filtered.items():\n",
    "    final_data = final_data[final_data[col] <= upper_limit]\n",
    "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Drop null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 193 rows\n"
     ]
    }
   ],
   "source": [
    "# Deal with null values. We allow categorical variables to be null\n",
    "# OTHER than grade, which is a particularly important categorical.\n",
    "# All non-categorical variables must be non-null, and we drop rows that do not meet this requirement\n",
    "required_cols = set(cols_to_pick) - set(cat_cols) - set([\"id\"])\n",
    "required_cols.add(\"grade\")\n",
    "\n",
    "n_rows = len(final_data)\n",
    "final_data.dropna(subset = required_cols ,inplace=True)\n",
    "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the columns selected will not be used directly in the model, but will be used to generate other features.\n",
    "# Create variables specifying the features that will be used\n",
    "\n",
    "# All categorical columns other than \"loan_status\" will be used as discrete features\n",
    "discrete_features = list(set(cat_cols) - set([\"loan_status\"]))\n",
    "\n",
    "# All numeric columns will be used as continuous features\n",
    "continuous_features = list(float_cols + perc_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181545, 25)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 5 - Save a Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the total_pymnt from the list of continuous features; this variable is highly predictive of the outcome but is not known at the time the loan is issued\n",
    "continuous_features = [i for i in continuous_features if i not in [\"total_pymnt\", \"recoveries\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output path for the pickle\n",
    "pickle_file = \"/\".join(['.', \"PickleData\", \"clean_data_feature.pickle\"])\n",
    "os.makedirs(os.path.dirname(pickle_file), exist_ok=True)\n",
    "pickle.dump( [final_data, discrete_features, continuous_features], open(pickle_file, \"wb\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Drop of data is 23%\n"
     ]
    }
   ],
   "source": [
    "Difference = len(data_now) - len(final_data)\n",
    "Percent_delta = Difference*100/len(data_now)\n",
    "print('The Drop of data is ' + str(round(Percent_delta)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv('C:\\\\Users\\\\ly266e\\\\Documents\\\\Training\\\\CMU\\\\Master\\\\Fall 2023 Mini 7\\\\Business_Analytics\\\\HW\\\\HW5\\\\Update\\\\Data\\\\clean_file_feature.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Read from Pickle if Saved\n",
    " Read data from saved pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the data and features from the pickle\n",
    "final_data, discrete_features, continuous_features = pickle.load( open( \"./PickleData/clean_data_feature.pickle\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181545, 25)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
